{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T21:09:31.700871Z",
     "start_time": "2020-02-01T21:09:25.740724Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "import ast\n",
    "#spacy dependencies\n",
    "import re \n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.matcher import Matcher\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from statistics import mean, median, mode\n",
    "import unicodedata\n",
    "import pyLDAvis.gensim\n",
    "from spacy import displacy\n",
    "import scattertext as st\n",
    "import inflect\n",
    "import scattertext as st\n",
    "from pprint import pprint\n",
    "import explacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T21:30:16.407866Z",
     "start_time": "2020-02-01T21:30:16.004364Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df = pd.read_csv('tokenized_model.csv')\n",
    "model_df = model_df.where((pd.notnull(model_df)), None)\n",
    "model_df.set_index('title',inplace=True)\n",
    "model_df.drop_duplicates(subset='name',inplace=True)\n",
    "model_plots = model_df['plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Lemmatizer and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:15:58.749944Z",
     "start_time": "2020-02-02T04:15:58.698336Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T00:34:41.301107Z",
     "start_time": "2020-02-01T21:35:52.531503Z"
    },
    "code_folding": [
     5,
     11,
     50,
     61
    ]
   },
   "outputs": [],
   "source": [
    "model_tokens = []\n",
    "\n",
    "#tokenization the text and reinstantiated the function each time it is used to \n",
    "#prevent entity merging errors\n",
    "for index,item in enumerate(model_plots):\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        punctuations = string.punctuation\n",
    "\n",
    "        #remove accendted characters\n",
    "        def remove_accented_characters(text):\n",
    "            cleantext = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            return cleantext\n",
    "\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        def spacy_tokenizer(row):\n",
    "\n",
    "            cleantext = re.sub(r'((?<!\\d)(\\d{1,3}|\\d{5,})(?!\\d))|([^a-zA-Z0-9 -])','',row)\n",
    "            cleantext2 = re.sub(r'(--)',' ', cleantext)\n",
    "            #remove accented characters\n",
    "            remove_accented_characters(cleantext2)\n",
    "\n",
    "            # Creating our token object using spacy\n",
    "            mytokens = nlp(cleantext2)\n",
    "\n",
    "            #merge entity names into one token\n",
    "            merge_nps = nlp.create_pipe(\"merge_entities\")\n",
    "            nlp.add_pipe(merge_nps)\n",
    "            mytokens = nlp(cleantext2)\n",
    "\n",
    "            # Lemmatizing each token and converting each token into lowercase\n",
    "            mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "            # Removing stop words\n",
    "            mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "            #remove all one letter words\n",
    "            mytokens = [ word for word in mytokens if len(word)>1 ]\n",
    "            \n",
    "            #create single string of all tokens\n",
    "            model_tokens.append(' '.join(mytokens))\n",
    "\n",
    "            # return preprocessed list of tokens\n",
    "            return mytokens\n",
    "\n",
    "        spacy_tokenizer(item)\n",
    "\n",
    "        print(f'{index} worked')\n",
    "    except:\n",
    "        model_tokens.append('Drop me!')\n",
    "        print(f'{index} didnt work')\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:17:42.326298Z",
     "start_time": "2020-02-02T04:17:42.103810Z"
    }
   },
   "outputs": [],
   "source": [
    "#merge all columns into one string for vectorization\n",
    "model_df['finished_data'] = model_df['score'].map(str) + ' ' + model_df['media_type'].map(str) + ' ' + model_df['Action'].map(str) + ' ' + model_df['Anime'].map(str) + ' ' + model_df['Children'].map(str) + ' ' + model_df['Comedy'].map(str) + ' ' + model_df['Cult'].map(str) + ' ' + model_df['Documentaries'].map(str) + ' ' + model_df['Dramas'].map(str) + ' ' + model_df['Fantasy'].map(str) + ' ' + model_df['Horror'].map(str) + ' ' + model_df['International'].map(str) + ' ' + model_df['Reality'].map(str) + ' ' + model_df['Sci-Fi'].map(str) + ' ' + model_df['Thrillers'].map(str) + ' ' + model_df['Romantic'].map(str) + ' ' + model_df['tokens']\n",
    "\n",
    "# remove nullvalues from each entry\n",
    "clean_finished_data = []\n",
    "finished_data = model_df['finished_data']\n",
    "for entry in finished_data:\n",
    "    clean_entry = entry.replace('None ', '').lower()\n",
    "    clean_finished_data.append(clean_entry)\n",
    "    \n",
    "model_df['finished_data'] = clean_finished_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:17:49.641630Z",
     "start_time": "2020-02-02T04:17:49.637845Z"
    }
   },
   "outputs": [],
   "source": [
    "#this may be where specify to join proper nouns if there are two next to each other \n",
    "count = CountVectorizer(max_df = .7, min_df = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:17:54.471195Z",
     "start_time": "2020-02-02T04:17:53.369171Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "count_matrix = count.fit_transform(model_df['finished_data'])\n",
    "pickle.dump(count_matrix, open('rec_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:17:59.426764Z",
     "start_time": "2020-02-02T04:17:59.414425Z"
    }
   },
   "outputs": [],
   "source": [
    "rec_matrix = pickle.load(open('rec_matrix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine them into on gian matrix and just say if movie then recommend book and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:18:10.110253Z",
     "start_time": "2020-02-02T04:18:03.982601Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating a cosign similarity matrix\n",
    "cosine_sim = cosine_similarity(rec_matrix, rec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:18:16.456948Z",
     "start_time": "2020-02-02T04:18:14.265002Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(cosine_sim, open('rec_cosine_sim.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the pickled matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:19:15.591527Z",
     "start_time": "2020-02-02T04:19:11.368913Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_sim = pickle.load(open('rec_cosine_sim.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:53.449378Z",
     "start_time": "2020-02-02T04:22:53.427926Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#sample function to get the movie information along with its title\n",
    "indices = pd.Series(model_df.index)\n",
    "df_names = list(model_df['name'])\n",
    "\n",
    "def recommendation(title, input_media_type,cosine_sim=cosine_sim):\n",
    "    recommendations = []\n",
    "    \n",
    "    #getting the index of the movie that matches the title\n",
    "    \n",
    "    #If the desired output is a movie return a movie\n",
    "    idx = indices[indices == title].index[0]\n",
    "\n",
    "    #creating a series with the similarty scores in descending order\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending=False)\n",
    "    \n",
    "    top_10_indexes = list(score_series.iloc[1:1000].index)\n",
    "        \n",
    "    #wcreate a unique id for each entry and write a search function\n",
    "    for item in top_10_indexes:\n",
    "        recommendations.append(list(model_df.index)[item])\n",
    "    \n",
    "    #only get movies if a movie is asked for\n",
    "    if input_media_type == 'movie':\n",
    "        movie_recs = []\n",
    "        for index1,rec in enumerate(recommendations):\n",
    "            movie_title = rec\n",
    "            for index2,name in enumerate(df_names):\n",
    "                if name == movie_title:\n",
    "                    movie_index = index2\n",
    "                    searched_movie_title = model_df['name'][movie_index]\n",
    "                    categorization = model_df['media_type'][movie_index]\n",
    "                    if categorization != 'movie':\n",
    "                        pass\n",
    "                    else:\n",
    "                        film_title = model_df['name'][movie_index]\n",
    "                        film_score = model_df['score'][movie_index]\n",
    "                        selected_image = model_df['image'][movie_index] \n",
    "                        selected_plot = model_df['plot'][movie_index]\n",
    "                        movie_recs.append(searched_movie_title)\n",
    "        return movie_recs[:5]\n",
    "    \n",
    "    #only get tv shows if a tv show is asked for\n",
    "    if input_media_type == 'tv':\n",
    "        tv_recs = []\n",
    "        for index1,rec in enumerate(recommendations):\n",
    "            tv_title = rec\n",
    "            for index2,name in enumerate(df_names):\n",
    "                if name == tv_title:\n",
    "                    tv_index = index2\n",
    "                    searched_tv_title = model_df['name'][tv_index]\n",
    "                    categorization = model_df['media_type'][tv_index]\n",
    "                    if categorization != 'tv':\n",
    "                        pass\n",
    "                    else:\n",
    "                        tv_title = model_df['name'][tv_index]\n",
    "                        tv_score = model_df['score'][tv_index]\n",
    "                        selected_tv_image = model_df['image'][tv_index] \n",
    "                        selected_tv_plot = model_df['plot'][tv_index]\n",
    "                        tv_recs.append(searched_tv_title)\n",
    "        return tv_recs[:5]\n",
    "    \n",
    "    #only get books if a book is asked for\n",
    "    if input_media_type == 'book':\n",
    "        book_recs = []\n",
    "        for index1,rec in enumerate(recommendations):\n",
    "            book_title = rec\n",
    "            for index2,name in enumerate(df_names):\n",
    "                if name == book_title:\n",
    "                    book_index = index2\n",
    "                    searched_book_title = model_df['name'][book_index]\n",
    "                    categorization = model_df['media_type'][book_index]\n",
    "                    if categorization != 'book':\n",
    "                        pass\n",
    "                    else:\n",
    "                        book_title = model_df['name'][book_index]\n",
    "                        book_score = model_df['score'][book_index]\n",
    "                        selected_book_image = model_df['image'][book_index] \n",
    "                        selected_book_plot = model_df['plot'][book_index]\n",
    "                        book_recs.append(searched_book_title)\n",
    "        return book_recs[:5]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:22:56.726798Z",
     "start_time": "2020-02-02T04:22:54.794300Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendation('Van Helsing', 'book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:27:46.196168Z",
     "start_time": "2020-02-02T04:27:46.187213Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_entities_plot(title):\n",
    "    try:\n",
    "        names = model_df['name']\n",
    "        plots = model_df['plot']\n",
    "        for index,name in enumerate(names):\n",
    "            if name == title:\n",
    "                book_index = index\n",
    "                plot_text = plots[book_index]\n",
    "        \n",
    "        #Visualize the entities\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        doc = nlp(plot_text)\n",
    "        spacy.displacy.serve(doc, style='ent')\n",
    "    except:\n",
    "        print('That name is not in the directory!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:27:55.755992Z",
     "start_time": "2020-02-02T04:27:49.160625Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_entities_plot('The Da Vinci Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitzation breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:27:59.616281Z",
     "start_time": "2020-02-02T04:27:59.611112Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_lemmitization(title):\n",
    "    try:\n",
    "        names = model_df['name']\n",
    "        plots = model_df['plot']\n",
    "        for index,name in enumerate(names):\n",
    "            if name == title:\n",
    "                media_index = index\n",
    "                plot_text = plots[media_index]\n",
    "        \n",
    "        #Visualize the entities\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        explacy.print_parse_info(nlp, plot_text)\n",
    "    except:\n",
    "        print('That name is not in the directory!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:28:02.867010Z",
     "start_time": "2020-02-02T04:28:02.050414Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_lemmitization('Narcos: Mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:28:25.826193Z",
     "start_time": "2020-02-02T04:28:06.809016Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "corpus = st.CorpusFromPandas(model_df,\n",
    "                             category_col='media_type',\n",
    "                             text_col='tokens',\n",
    "                             nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms most associated with books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:28:25.829602Z",
     "start_time": "2020-02-02T04:28:11.326Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(corpus.get_scaled_f_scores_vs_background().index[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the categories of the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T21:52:55.313721Z",
     "start_time": "2020-01-21T21:52:55.293879Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_builder = st.FeatsFromOnlyEmpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T21:56:07.026446Z",
     "start_time": "2020-01-21T21:54:08.839130Z"
    }
   },
   "outputs": [],
   "source": [
    "empath_corpus = st.CorpusFromParsedDocuments(model_df,\n",
    "                                             category_col='media_type',\n",
    "                                             feats_from_spacy_doc=feat_builder,\n",
    "                                             parsed_col='tokens').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T21:57:26.151285Z",
     "start_time": "2020-01-21T21:56:39.820591Z"
    }
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(empath_corpus,\n",
    "                                       category='book',\n",
    "                                       category_name='Books',\n",
    "                                       not_category_name='Netflix',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       metadata=model_df['media_type'],\n",
    "                                       use_non_text_features=True,\n",
    "                                       use_full_doc=True,\n",
    "                                       topic_model_term_lists=feat_builder.get_top_model_term_lists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T22:10:49.541237Z",
     "start_time": "2020-01-21T22:10:49.504051Z"
    }
   },
   "outputs": [],
   "source": [
    "open(\"book-v-netflix-empathy.html\", 'wb').write(html.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

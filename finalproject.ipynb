{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T01:56:05.452566Z",
     "start_time": "2020-02-01T01:55:59.062626Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "import ast\n",
    "#spacy dependencies\n",
    "import re \n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spacy.matcher import Matcher\n",
    "from gensim import corpora\n",
    "import pickle\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from statistics import mean, median, mode\n",
    "import unicodedata\n",
    "import pyLDAvis.gensim\n",
    "from spacy import displacy\n",
    "import scattertext as st\n",
    "import inflect\n",
    "import scattertext as st\n",
    "from pprint import pprint\n",
    "import explacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T00:45:09.702602Z",
     "start_time": "2020-02-01T00:45:09.433272Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_df = pd.read_csv('formatted_movies.csv')\n",
    "tv_df = pd.read_csv('formatted_tv.csv')\n",
    "books_df = pd.read_csv('books.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work with genre list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T21:30:01.195203Z",
     "start_time": "2020-01-22T21:30:00.917749Z"
    }
   },
   "outputs": [],
   "source": [
    "#TV\n",
    "full_tv_genres = list(tv_df['genres'])\n",
    "all_tv_genres = []\n",
    "complete_tv_genres = []\n",
    "action_tf = []\n",
    "\n",
    "for entry in full_tv_genres:\n",
    "    all_tv_genres.append(entry)\n",
    "    \n",
    "tv_range = list(range(0,1723))\n",
    "all_clean_tv_genres = []\n",
    "for entry in all_tv_genres:\n",
    "    entry = ast.literal_eval(entry)\n",
    "    complete_tv_genres.append(entry)\n",
    "    \n",
    "entire_tv_list = []\n",
    "for tv_group in complete_tv_genres:\n",
    "    for show in tv_group:\n",
    "        entire_tv_list.append(show)\n",
    "\n",
    "tv_set = set(entire_tv_list)\n",
    "\n",
    "\n",
    "#Movies\n",
    "full_movie_genres = list(movie_df['genres'])\n",
    "all_movie_genres = []\n",
    "complete_movie_genres = []\n",
    "\n",
    "for entry in full_movie_genres:\n",
    "    all_movie_genres.append(entry)\n",
    "    \n",
    "movie_range = list(range(0,1912))\n",
    "all_clean_movie_genres = []\n",
    "for entry in all_movie_genres:\n",
    "    entry = ast.literal_eval(entry)\n",
    "    complete_movie_genres.append(entry)\n",
    "        \n",
    "entire_movie_list = []\n",
    "for movie_group in complete_movie_genres:\n",
    "    for movie in movie_group:\n",
    "        entire_movie_list.append(movie)\n",
    "        \n",
    "movie_set = set(entire_movie_list)\n",
    "\n",
    "#Books\n",
    "full_book_genres = list(books_df['genres'])\n",
    "all_book_genres = []\n",
    "complete_book_genres = []\n",
    "\n",
    "for entry in full_book_genres:\n",
    "    all_book_genres.append(entry)\n",
    "    \n",
    "book_range = list(range(len(books_df)))\n",
    "all_clean_book_genres = []\n",
    "for entry in all_book_genres:\n",
    "    entry = ast.literal_eval(entry)\n",
    "    complete_book_genres.append(entry)\n",
    "        \n",
    "entire_book_list = []\n",
    "for book_group in complete_book_genres:\n",
    "    for book in book_group:\n",
    "        entire_book_list.append(book)\n",
    "        \n",
    "book_set = set(entire_book_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupings for book genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T21:30:04.141933Z",
     "start_time": "2020-01-22T21:30:04.066294Z"
    },
    "code_folding": [
     0,
     67,
     75,
     83,
     92,
     101,
     110,
     123,
     141,
     147,
     169,
     179
    ]
   },
   "outputs": [],
   "source": [
    "reality = ['17th Century','18th Century','19th Century',\n",
    "           '20th Century','21st Century','Abuse','Academia','Academic',\n",
    "           'Activism','Adoption','Adult','Aeroplanes','African American',\n",
    "           'African Literature','Albanian Literature','Alcohol','Algeria',\n",
    "           'Amazon',' American Civil War','American Fiction',\n",
    "           'American History','American Revolution',\n",
    "           'American Revolutionary War','Americana','Amish',\n",
    "           'Ancient','Ancient History','Angels','Angola','Anthologies',\n",
    "           'Anthropology','Antisemitism','Apple','Archaeology',\n",
    "           'Architecture','Art','Art Design','Art History',\n",
    "           'Art and Photography','Arthurian','Artificial Intelligence',\n",
    "           'Asia','Asian Literature','Astronomy','Atheism','Australia',\n",
    "           'Autobiography','Aviation','Bande Dessin√©e','Bangladesh',\n",
    "           'Banned Books', 'Baseball','Basketball','Biblical Fiction',\n",
    "           'Biography','Biography Memoir','Biology','Birds','Books About Books',\n",
    "           'Botswana','Brain','Brazil','British Literature','Buddhism',\n",
    "           'Buisness','Bulgaria','Business','Canada','Canadian Literature',\n",
    "           'Cartography','Catholic','Cats','Central Africa','Chapter Books',\n",
    "           'China','Chinese Literature','Christian','Christian Fantasy',\n",
    "           'Christian Living','Christian Non Fiction','Christianity','Church',\n",
    "           ' Church History',' Cities',' Civil War',' Classical Studies',' Classics',\n",
    "           ' Clean Romance ',' Climate Change',' Climbing',' Coding',' Collections',\n",
    "           'Communication',' Computer Science',' Computers',' Conservation',' Conspiracy Theories',\n",
    "           ' Cookbooks',' Cooking',' Counselling','Crafts',' Crime',' Criticism',' Culinary',\n",
    "           ' Cults',' Cultural',' Cultural Studies',' Culture',' Currency','Cycling',' Czech Literature',\n",
    "           ' Danish','Death',' Denmark',' Design','Diary',' Dinosaurs',' Disability',' Disability Studies',\n",
    "           ' Discipleship',' Doctors','Drawing',' Dutch Literature',' Dystopia',' Earth Sciences',\n",
    "           ' Eastern Africa',' Ecology',' Economics',' Education',' Edwardian',' Egypt',\n",
    "           ' Emergency Services',' Emotion',' Engineering',' English History',' Entrepreneurship',\n",
    "           ' Environment','Essays',' Ethiopia',' European History',' European Literature',\n",
    "           ' Evangelism',' Evolution','Faith',' Family',' Family Law',' Feminism',' Feminist Theory','Film',\n",
    "           ' Finance',' Finnish Literature',' Fitness',' Folk Tales',' Folklore','Food History',\n",
    "           ' Food Writing',' Food and Drink',' Food and Wine','French Literature',' French Revolution','Game Design',\n",
    "           ' Games',' Gaming',' Gardening',' Gay',' Geek',' Gender',' Gender Studies',' Gender and Sexuality',\n",
    "           ' Genetics',' Geography',' Geology',' German Literature',' Germany',' Ghana','God',' Gods',' Google',\n",
    "           'Government','Greece',' Greek Mythology',' Green',' Health',' Health Care',' Historical',\n",
    "           'History Of Science',' Holocaust',' Horse Racing','Humanities',' Hungarian Literature',\n",
    "           ' Hungary',' India',' Indian Literature',' Inspirational',' International Development ',\n",
    "           ' International Literature',' International Relations',' Internet',' Interracial Romance',' Iran',\n",
    "           ' Ireland',' Irish Literature',' Islam',' Israel',' Italian Literature',' Italy',' Japan',\n",
    "           ' Japanese History',' Japanese Literature',' Jazz','Jewish',' Journal',' Journalism',' Judaica',\n",
    "           ' Judaism',' Juvenile',' Kenya','Labor',' Language',' Latin American',' Law',' Lds',\n",
    "           ' Lds Fiction',' Leadership',' Lebanon','Lesbian',' Linguistics',' Literary Criticism',\n",
    "           ' Literary Fiction',' Literature','Madagascar','Magical Realism',' Management',' Managers',' Maps',' Marathi',\n",
    "           ' Marriage',' Martial Arts','Mathematics',' Media Tie In','Medicine',' Medieval',' Medieval History',\n",
    "           'Memoir',' Mental Health',' Mental Illness',' Metaphysics',' Microhistory','Military',' Modern',\n",
    "           ' Money','Mormonism',' Morocco',' Moscow',' Mountaineering',' Movies',' Murder Mystery',' Music',\n",
    "           ' Music Biography',' Musicals',' Muslimah',' Muslims',' Native Americans',' Natural History',\n",
    "           ' Nature',' Naval History',' Neuroscience',' New Adult',' New Age',' New Testament',' New Weird',\n",
    "           ' New York',' Nigeria',' Nobel Prize',' Nonfiction','North American History',' Northern Africa',\n",
    "           ' Nursing',' Nutrition',' Old Testament',' Omegaverse',' Ornithology',' Own',' Paganism',' Pakistan',\n",
    "           ' Palaeontology','Personal Development',' Personal Finance',' Philosophy',' Photography',' Physics',' Picture Books',\n",
    "           ' Plays',' Poetry',' Poland',' Polish Literature',' Political Science',' Politics',' Polyamorous',\n",
    "           ' Polyamory',' Polygamy',' Pop Culture',' Popular Science',' Portugal',' Portuguese Literature',\n",
    "           ' Prehistory',' Presidents','Programming',' Religion',' Research','Roman','Romania','Russia',\n",
    "           ' Russian History',' Russian Literature',' Russian Revolution',' Rwanda',' Scandinavian Literature'\n",
    "           ,' Scandinavian Literature',' Scotland',' Self Help',' Senegal','Sierra Leone',' Social Issues',\n",
    "           ' Social Justice',' Social Media',' Social Movements',' Social Science',' Social Work',\n",
    "           ' Society',' Sociology',' South Africa',' Southern',' Southern Africa',' Southern Gothic',\n",
    "           ' Soviet Union',' Space',' Spain',' Spanish Literature','Spirituality','Sudan',' Sustainability',' Sweden',\n",
    "           ' Swedish Literature',' Tasmania',' Teaching',' Taoism',' Textbooks',' The United States Of America',\n",
    "           ' Theatre',' Theology',' Theory','Transport',' Travel',' Travelogue',' Tudor Period',\n",
    "           ' Turkish',' Turkish Literature','Uganda',' Ukraine',' United States','Vegan',' Vegetarian',' War',\n",
    "           ' Warfare','Western Africa',' Wilderness',' Wildlife','Womens Studies',' World History',\n",
    "           ' World War I',' World War II',' Writing',' Yaoi',' Zimbabwe']\n",
    "clean_reality = [x.strip() for x in reality]\n",
    "\n",
    "anime = ['Comic Book',' Comic Fantasy',' Comic Strips',' Comics',\n",
    "         ' Comics Manga',' Drawing',' Manga']\n",
    "clean_anime = [x.strip() for x in anime]\n",
    "\n",
    "classics = ['Classics']\n",
    "\n",
    "comedy = ['Cartoon','Comedy','Humor']\n",
    "\n",
    "action = ['Action','Adventure','Aliens',' Apocalyptic',' Batman','Combat',\n",
    "          ' Dc Comics',' Doctor Who',' Dungeons and Dragons',' Epic',\n",
    "          ' Epic Fantasy',' Ghost Stories',' Ghosts','Magic',' Martial Arts',\n",
    "          ' Marvel','Monsters','Racing',' Robots',' Star Trek',' Star Wars',\n",
    "          'Superheroes',' Superman',' Supernatural',' Survival',' Sword and Sorcery',\n",
    "          ' Vampires','Witches',' Wolves',' Zombies']\n",
    "clean_action = [x.strip() for x in action]\n",
    "\n",
    "adventure = ['      Adventure',' Animal Fiction ' ,'Apocalyptic    ',' Bizarro Fiction'  ,\n",
    "             'College  ',' Coming Of Age  ',' Cyberpunk  ',' Doctor Who  '  ,\n",
    "             ' Dragons  ',' Dungeons and Dragons    ',' Dystopia    ',' Fables  ',\n",
    "             ' Fairies  ','Fantasy   ',' Heroic Fantasy  ',' Marvel    ',\n",
    "             'Military Fiction  ',' Monsters    ',' Pirates  ',' Star Trek ',\n",
    "             ' Superheroes    ',' Superman    ',' Time Travel  ',\n",
    "             ' Time Travel Romance  ',' Wildlife    ']\n",
    "clean_adventure = [x.strip() for x in adventure]\n",
    "\n",
    "children = ['Beauty and The Beast ','Childrens','Cinderella ',\n",
    "            'Comic Book',' Comic Fantasy',' Comic Strips',' Comics',\n",
    "            ' Comics Manga',' Coming Of Age','Dc Comics',' Doctor Who',\n",
    "            'Dungeons and Dragons',' Musicals ',' Picture Books','Star Trek',\n",
    "            ' Star Wars',' Teen ','Young Adult',' Young Adult Contemporary',\n",
    "            ' Young Adult Fantasy ',' Young Adult Historical Fiction',\n",
    "            ' Young Adult Paranormal ',' Young Adult Romance ',' Young Readers ']\n",
    "clean_children = [x.strip() for x in children]\n",
    "\n",
    "documentaries = [' Alternate History','American History ','Ancient History  ',\n",
    "                 'Art History  ','Church History  ',' English History ',\n",
    "                 ' European History  ',' Food History  ',' History  ',\n",
    "                 ' History Of Science  ','Japanese History ','Medieval History  ',\n",
    "                 'Microhistory  ','Military History',' Natural History  ',\n",
    "                 'Naval History  ',' North American History  ','Prehistory  ',\n",
    "                 ' Russian History  ',' World History  ']\n",
    "clean_docs = [x.strip() for x in documentaries]\n",
    "\n",
    "dramas = [' Adult Fiction ',' Apocalyptic ',' Cyberpunk',' Dc Comics    ',\n",
    "          ' Dragons  ',' Dungeons and Dragons ',' High School',\n",
    "          ' Monsters    ',' Murder Mystery',' Mystery',\n",
    "          ' Mystery Thriller',' Police',' Shapeshifters',\n",
    "          ' Space Opera','Spy Thriller',' Star Trek   ',\n",
    "          ' Star Wars    ',' Superheroes ',' Supernatural  ',\n",
    "          ' Survival  ',' Sword and Sorcery  ',' Thriller',\n",
    "          ' Tragedy',' True Crime',' Vampires  ',' Victor Frankenstein',\n",
    "          'War  ',' Warfare  ',' Werewolves',' World War I  ',\n",
    "          ' World War II  ',' Young Adult Fantasy  ',\n",
    "          'Young Adult Paranormal  ',' Young Adult Romance  ']\n",
    "clean_dramas = [x.strip() for x in dramas]\n",
    "\n",
    "fantasy = ['Animal Fiction   ','Batman ',' Christian Fantasy ',\n",
    "           ' Cinderella',' Comic Fantasy ',' Cyberpunk ',\n",
    "           ' Dark Fantasy',' Dc Comics',' Doctor Who ',' Dragons ',\n",
    "           ' Dungeons and Dragons',' Epic Fantasy ',\n",
    "           ' Fables - Adventure',' Fae',' Fairies ',\n",
    "           ' Fairy Tale Retellings',' Fairy Tales',' Fantasy  ',\n",
    "           ' Fantasy Romance','Folk Tales ',' Folklore ',' Futurism',\n",
    "           ' Ghost Stories',' Greek Mythology',' Heroic Fantasy ',\n",
    "           ' High Fantasy',' Historical Fantasy',' M M Fantasy',\n",
    "           ' Magic ',' Manga',' Mermaids',' Monsters ',' Robots ',\n",
    "           ' Science Fiction',' Science Fiction Fantasy',\n",
    "           ' Shapeshifters',' Star Trek ',' Star Wars',\n",
    "           ' Superheroes',' Superman ',' Supernatural',\n",
    "           ' Sword and Sorcery',' Time Travel' ,\n",
    "           ' Time Travel Romance',' Urban Fantasy',' Vampires ',\n",
    "           ' Werewolves',' Young Adult Fantasy']\n",
    "clean_fantasy = [x.strip() for x in fantasy]\n",
    "\n",
    "horror = [' Demons ',' Dystopia','Ghost Stories',\n",
    "          ' Ghosts','  Gothic',' Horror',' Monsters',\n",
    "          ' True Crime',' Vampires ',' Victor Frankenstein',\n",
    "          ' Werewolves',' Witches']\n",
    "clean_horror = [x.strip() for x in horror]\n",
    "\n",
    "international = ['Africa    ',' Algeria    ',' Amazon    ',\n",
    "                 ' American    ',' Angola    ',' Asia    ',\n",
    "                 'Australia    ',' Bangladesh    ',' Botswana    ',\n",
    "                 ' Brazil    ',' Bulgaria    ',' Canada    ',\n",
    "                 ' Central Africa    ',' China    ',' Danish    ',\n",
    "                 ' Denmark    ',' Eastern Africa    ',' Egypt    ',\n",
    "                 ' Ethiopia    ',' France    ',' Germany    ',\n",
    "                 ' Ghana    ',' Greece    ','Hungary    ',' India    ',\n",
    "                 ' Iran    ',' Ireland    ',' Israel    ','Italy    ',\n",
    "                 ' Japan    ',' Kenya    ',' Lebanon     ',\n",
    "                 ' Madagascar    ',' Morocco  '  ,' Moscow    ',\n",
    "                 ' Nigeria    ',' Northern Africa    ',' Pakistan    ',\n",
    "                 ' Poland    ',' Roman    ',' Romania    ',\n",
    "                 ' Russia    ',' Rwanda    ',' Scotland    ',' Sierra Leone    ',\n",
    "                 'South Africa    ',' Southern    ',' Southern Africa    ',\n",
    "                 ' Soviet Union    ',' Spain    ',' Sudan    ',\n",
    "                 ' Sweden    ',' The United States Of America    ',\n",
    "                 ' Turkish    ',' Uganda    ',' Ukraine    ',\n",
    "                 ' United States    ',' Western Africa    ',\n",
    "                 ' Westerns  ',' Zimbabwe  ']\n",
    "clean_international = [x.strip() for x in international]\n",
    "\n",
    "romantic = ['BDSM ',' Christian Romance ' ,' Cinderella    Fantasy',\n",
    "            ' Erotic Romance',' Historical Romance',\n",
    "            ' Interracial Romance  ',' Lovecraftian',\n",
    "            ' Medieval Romance ',' Military Romance',\n",
    "            ' Paranormal Romance',' Regency Romance',\n",
    "            'Romance',' Romania  ',\n",
    "            ' Science Fiction Romance',' Sports Romance',\n",
    "            ' Western Romance']\n",
    "clean_romantic = [x.strip() for x in romantic]\n",
    "\n",
    "scifi = [ 'Animal Fiction', 'Artificial Intelligence  ',' Cyberpunk',\n",
    "         'Dark Fantasy ',' Demons',' Doctor Who ',' Dragons ',\n",
    "         ' Dungeons and Dragons',' Epic Fantasy ',' Futurism',\n",
    "         ' Futuristic ',' Ghost Stories ',' Ghosts ',\n",
    "         ' Hard Science Fiction ',' Magic ',' Paranormal',\n",
    "         ' Paranormal Mystery ',' Paranormal Romance',' Popular Science ',\n",
    "         ' Science Fiction',' Science Fiction Fantasy',\n",
    "         ' Science Fiction Romance',' Space Opera',' Star Trek ',\n",
    "         ' Star Wars',' Superheroes',' Superman',' Supernatural',\n",
    "         ' Unicorns',' Utopia',' Victor Frankenstein',\n",
    "         ' Weird Fiction',' Werewolves',' Zombies']\n",
    "clean_scifi = [x.strip() for x in scifi]\n",
    "\n",
    "thriller = ['Aliens ','Cozy Mystery','Crime ',' Demons ',\n",
    "            'Detective','Dinosaurs ',' Dragons ',' Legal Thriller',\n",
    "            'Murder Mystery',' Mystery',' Mystery Thriller',\n",
    "            'Nordic Noir','Occult','Paranormal Mystery ',\n",
    "            ' Spy Thriller',' Thriller']\n",
    "clean_thriller = [x.strip() for x in thriller]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T00:47:04.018829Z",
     "start_time": "2020-02-01T00:47:03.545942Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df = pd.read_csv('tokenized_model.csv')\n",
    "model_df = model_df.where((pd.notnull(model_df)), None)\n",
    "model_df.set_index('title',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:40:45.939370Z",
     "start_time": "2020-01-22T22:40:45.215455Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = pd.value_counts(entire_movie_list).plot.barh(title='Netflix Movie Genre Distribution', figsize=(11,6), color='red');\n",
    "ax.set_xlabel('Number of movies')\n",
    "ax.set_ylabel('Genre')\n",
    "plt.xticks(rotation = 0)\n",
    "plt.savefig('netflix_movie_genres.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:40:53.237149Z",
     "start_time": "2020-01-22T22:40:52.860907Z"
    }
   },
   "outputs": [],
   "source": [
    "ax2 = pd.value_counts(entire_tv_list).plot.barh(title='Netflix TV Genre Distribution', figsize=(11,6), color='red')\n",
    "ax2.set_xlabel('Number of shows')\n",
    "ax2.set_ylabel('Genre')\n",
    "plt.savefig('netflix_tv_genres.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:41:01.142129Z",
     "start_time": "2020-01-22T22:41:00.882084Z"
    }
   },
   "outputs": [],
   "source": [
    "ax3 = (model_df['media_type'].value_counts()).plot.barh(title='Media Distribution', color='red', figsize=(11,6))\n",
    "ax3.set_xlabel('Number of Instances')\n",
    "ax3.set_ylabel('Media Type')\n",
    "plt.savefig('media_distribution.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T23:54:50.410628Z",
     "start_time": "2020-01-22T23:54:50.135342Z"
    }
   },
   "outputs": [],
   "source": [
    "ax4 = (model_df['score'].value_counts()).plot.barh(title='Approximate Ratings', color='red', figsize=(11,6))\n",
    "ax4.set_ylabel('Approximate Rating')\n",
    "ax4.set_xlabel('Number of Instances')\n",
    "plt.savefig('rounded_ratings.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T21:31:38.872951Z",
     "start_time": "2020-01-22T21:31:38.809689Z"
    }
   },
   "outputs": [],
   "source": [
    "books_df['International'].value_counts()\n",
    "books_international_count = 279\n",
    "books_df['Dramas'].value_counts()\n",
    "books_dramas_count = 721\n",
    "books_df['Comedy'].value_counts()\n",
    "books_comedy_count = 128\n",
    "books_df['Children'].value_counts()\n",
    "books_children_count = 252\n",
    "books_df['Documentaries'].value_counts()\n",
    "books_documentaries_count = 167\n",
    "books_df['Romantic'].value_counts()\n",
    "books_romantic_count = 399\n",
    "books_df['Reality'].value_counts()\n",
    "books_reality_count = 2627\n",
    "books_df['Anime'].value_counts()\n",
    "books_anime_count = 21\n",
    "books_df['Cult'].value_counts()\n",
    "books_cult_count = 9\n",
    "books_df['Action'].value_counts()\n",
    "books_action_count = 382\n",
    "books_df['Adventure'].value_counts()\n",
    "books_adventure_count = 524\n",
    "books_df['Fantasy'].value_counts()\n",
    "books_fantasy_count = 893\n",
    "books_df['Horror'].value_counts()\n",
    "books_horror_count = 219\n",
    "books_df['Sci-Fi'].value_counts()\n",
    "books_scifi_count = 673\n",
    "books_df['Thriller'].value_counts()\n",
    "books_thriller_count = 496"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T23:54:59.714230Z",
     "start_time": "2020-01-22T23:54:59.410066Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.barh(['Reality', 'Anime', 'Cult', 'Comedy', \n",
    "         'Action', 'Adventure', 'Children',\n",
    "         'Documentaries', 'Dramas', 'Fantasy',\n",
    "         'Horror', 'International','Romantic', 'Sci-Fi', 'Thriller'], \n",
    "        [books_reality_count,books_anime_count,books_cult_count, \n",
    "         books_comedy_count,books_action_count,books_adventure_count,\n",
    "         books_children_count, books_documentaries_count,books_dramas_count,\n",
    "         books_fantasy_count,books_horror_count,books_international_count,\n",
    "         books_romantic_count,books_scifi_count, books_thriller_count],color='red')\n",
    "plt.ylabel('Genres')\n",
    "plt.xlabel('Number of books')\n",
    "plt.title(\"Goodreads Best of the 21's Century Genres\")\n",
    "plt.savefig('goodreads.png');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA on TV plot length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:28:00.857842Z",
     "start_time": "2020-01-22T22:28:00.844954Z"
    }
   },
   "outputs": [],
   "source": [
    "#TV\n",
    "tv_plot_lengths = []\n",
    "\n",
    "for plot in tv_df['plot']:\n",
    "    tv_plot_lengths.append(len(plot))\n",
    "\n",
    "average_tv_length = round(mean(tv_plot_lengths))\n",
    "median_tv_length = median(tv_plot_lengths)\n",
    "mode_tv_length = mode(tv_plot_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:28:01.371341Z",
     "start_time": "2020-01-22T22:28:01.357647Z"
    }
   },
   "outputs": [],
   "source": [
    "#Movies\n",
    "movie_plot_lengths = []\n",
    "\n",
    "for plot in movie_df['plot']:\n",
    "    movie_plot_lengths.append(len(plot))\n",
    "    \n",
    "average_movie_length = round(mean(movie_plot_lengths))\n",
    "median_movie_length = median(movie_plot_lengths)\n",
    "mode_movie_length = mode(movie_plot_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:28:01.982291Z",
     "start_time": "2020-01-22T22:28:01.963915Z"
    }
   },
   "outputs": [],
   "source": [
    "#Books\n",
    "book_plot_length = []\n",
    "for plot in books_df['text']:\n",
    "    book_plot_length.append(len(plot))\n",
    "\n",
    "average_book_length = round(mean(book_plot_length))\n",
    "median_book_length = round(median(book_plot_length))\n",
    "mode_book_length = mode(book_plot_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot length graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T22:49:09.123833Z",
     "start_time": "2020-01-22T22:49:08.506279Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [average_movie_length, average_tv_length, average_book_length]\n",
    "plt.bar(['Netflix movies', 'Netflix TV shows', 'Books'],texts,color='red')\n",
    "# fig.plot.bar(media_types,texts,color='red')\n",
    "plt.title('Average Plot Description Length');\n",
    "plt.ylabel('Source')\n",
    "plt.xlabel('Word count')\n",
    "plt.savefig('plot_length.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy Lemmatizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T15:59:07.787885Z",
     "start_time": "2020-01-21T15:59:07.782876Z"
    }
   },
   "outputs": [],
   "source": [
    "model_plots = model_df['plot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T07:04:27.931104Z",
     "start_time": "2020-01-21T04:10:55.454372Z"
    },
    "code_folding": [
     5,
     11,
     50,
     61
    ]
   },
   "outputs": [],
   "source": [
    "model_tokens = []\n",
    "\n",
    "#tokenization the text and reinstantiated the function each time it is used to \n",
    "#prevent entity merging errors\n",
    "for index,item in enumerate(model_plots):\n",
    "    try:\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        punctuations = string.punctuation\n",
    "\n",
    "        #remove accendted characters\n",
    "        def remove_accented_characters(text):\n",
    "            cleantext = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            return cleantext\n",
    "\n",
    "        matcher = Matcher(nlp.vocab)\n",
    "\n",
    "        def spacy_tokenizer(row):\n",
    "\n",
    "            cleantext = re.sub(r'((?<!\\d)(\\d{1,3}|\\d{5,})(?!\\d))|([^a-zA-Z0-9 -])','',row)\n",
    "            cleantext2 = re.sub(r'(--)',' ', cleantext)\n",
    "            #remove accented characters\n",
    "            remove_accented_characters(cleantext2)\n",
    "\n",
    "            # Creating our token object using spacy\n",
    "            mytokens = nlp(cleantext2)\n",
    "\n",
    "            #merge entity names into one token\n",
    "            merge_nps = nlp.create_pipe(\"merge_entities\")\n",
    "            nlp.add_pipe(merge_nps)\n",
    "            mytokens = nlp(cleantext2)\n",
    "\n",
    "            # Lemmatizing each token and converting each token into lowercase\n",
    "            mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "            # Removing stop words\n",
    "            mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "\n",
    "            #remove all one letter words\n",
    "            mytokens = [ word for word in mytokens if len(word)>1 ]\n",
    "            \n",
    "            #create single string of all tokens\n",
    "            model_tokens.append(' '.join(mytokens))\n",
    "\n",
    "            # return preprocessed list of tokens\n",
    "            return mytokens\n",
    "\n",
    "        spacy_tokenizer(item)\n",
    "\n",
    "        print(f'{index} worked')\n",
    "    except:\n",
    "        model_tokens.append('Drop me!')\n",
    "        print(f'{index} didnt work')\n",
    "        pass\n",
    "\n",
    "#merge all columns into one string for vectorization\n",
    "model_df['finished_data'] = model_df['score'].map(str) + ' ' + model_df['media type'].map(str) + ' ' + model_df['Action'].map(str) + ' ' + model_df['Anime'].map(str) + ' ' + model_df['Children'].map(str) + ' ' + model_df['Comedy'].map(str) + ' ' + model_df['Cult'].map(str) + ' ' + model_df['Documentaries'].map(str) + ' ' + model_df['Dramas'].map(str) + ' ' + model_df['Fantasy'].map(str) + ' ' + model_df['Horror'].map(str) + ' ' + model_df['International'].map(str) + ' ' + model_df['Reality'].map(str) + ' ' + model_df['Sci-Fi'].map(str) + ' ' + model_df['Thrillers'].map(str) + ' ' + model_df['Romantic'].map(str) + ' ' + model_df['tokens']\n",
    "\n",
    "# remove nullvalues from each entry\n",
    "clean_finished_data = []\n",
    "finished_data = model_df['finished_data']\n",
    "for entry in finished_data:\n",
    "    clean_entry = entry.replace('None ', '').lower()\n",
    "    clean_finished_data.append(clean_entry)\n",
    "    \n",
    "model_df['finished_data'] = clean_finished_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T02:33:06.659286Z",
     "start_time": "2020-01-22T02:33:06.454984Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#this may be where specify to join proper nouns if there are two next to each other \n",
    "\n",
    "count = CountVectorizer(max_df = .7, min_df = 3)\n",
    "count_matrix = count.fit_transform(model_df['finished_data'])\n",
    "pickle.dump(count_matrix, open('rec_matrix.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T21:10:34.080240Z",
     "start_time": "2020-01-23T21:10:34.051239Z"
    }
   },
   "outputs": [],
   "source": [
    "rec_matrix = pickle.load(open('rec_matrix.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine them into on gian matrix and just say if movie then recommend book and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T21:10:47.416232Z",
     "start_time": "2020-01-23T21:10:38.444073Z"
    }
   },
   "outputs": [],
   "source": [
    "#creating a cosign similarity matrix\n",
    "cosine_sim = cosine_similarity(rec_matrix, rec_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T02:33:34.855802Z",
     "start_time": "2020-01-22T02:33:32.716506Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(cosine_sim, open('rec_cosine_sim.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open the pickled matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T00:46:25.301111Z",
     "start_time": "2020-02-01T00:46:19.373031Z"
    }
   },
   "outputs": [],
   "source": [
    "cosine_sim = pickle.load(open('rec_cosine_sim.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T00:47:10.257512Z",
     "start_time": "2020-02-01T00:47:10.236173Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#sample function to get the movie information along with its title\n",
    "indices = pd.Series(model_df.index)\n",
    "df_names = list(model_df['name'])\n",
    "\n",
    "def recommendation(title, input_media_type,cosine_sim=cosine_sim):\n",
    "    recommendations = []\n",
    "    \n",
    "    #getting the index of the movie that matches the title\n",
    "    \n",
    "    #If the desired output is a movie return a movie\n",
    "    idx = indices[indices == title].index[0]\n",
    "\n",
    "    #creating a series with the similarty scores in descending order\n",
    "    score_series = pd.Series(cosine_sim[idx]).sort_values(ascending=False)\n",
    "    \n",
    "    top_10_indexes = list(score_series.iloc[1:1000].index)\n",
    "        \n",
    "    #wcreate a unique id for each entry and write a search function\n",
    "    for item in top_10_indexes:\n",
    "        recommendations.append(list(model_df.index)[item])\n",
    "    \n",
    "    #only get movies if a movie is asked for\n",
    "    if input_media_type == 'movie':\n",
    "        movie_recs = []\n",
    "        for index1,rec in enumerate(recommendations):\n",
    "            movie_title = rec\n",
    "            for index2,name in enumerate(df_names):\n",
    "                if name == movie_title:\n",
    "                    movie_index = index2\n",
    "                    searched_movie_title = model_df['name'][movie_index]\n",
    "                    categorization = model_df['media_type'][movie_index]\n",
    "                    if categorization != 'movie':\n",
    "                        pass\n",
    "                    else:\n",
    "                        film_title = model_df['name'][movie_index]\n",
    "                        film_score = model_df['score'][movie_index]\n",
    "                        selected_image = model_df['image'][movie_index] \n",
    "                        selected_plot = model_df['plot'][movie_index]\n",
    "                        movie_recs.append(searched_movie_title)\n",
    "        return movie_recs[:10]\n",
    "    \n",
    "    #only get tv shows if a tv show is asked for\n",
    "    if input_media_type == 'tv':\n",
    "        tv_recs = []\n",
    "        for index1,rec in enumerate(recommendations):\n",
    "            tv_title = rec\n",
    "            for index2,name in enumerate(df_names):\n",
    "                if name == tv_title:\n",
    "                    tv_index = index2\n",
    "                    searched_tv_title = model_df['name'][tv_index]\n",
    "                    categorization = model_df['media_type'][tv_index]\n",
    "                    if categorization != 'tv':\n",
    "                        pass\n",
    "                    else:\n",
    "                        tv_title = model_df['name'][tv_index]\n",
    "                        tv_score = model_df['score'][tv_index]\n",
    "                        selected_tv_image = model_df['image'][tv_index] \n",
    "                        selected_tv_plot = model_df['plot'][tv_index]\n",
    "                        tv_recs.append(searched_tv_title)\n",
    "        return tv_recs[:10]\n",
    "    \n",
    "    #only get books if a book is asked for\n",
    "    if input_media_type == 'book':\n",
    "        book_recs = []\n",
    "        for index1,rec in enumerate(recommendations):\n",
    "            book_title = rec\n",
    "            for index2,name in enumerate(df_names):\n",
    "                if name == book_title:\n",
    "                    book_index = index2\n",
    "                    searched_book_title = model_df['name'][book_index]\n",
    "                    categorization = model_df['media_type'][book_index]\n",
    "                    if categorization != 'book':\n",
    "                        pass\n",
    "                    else:\n",
    "                        book_title = model_df['name'][book_index]\n",
    "                        book_score = model_df['score'][book_index]\n",
    "                        selected_book_image = model_df['image'][book_index] \n",
    "                        selected_book_plot = model_df['plot'][book_index]\n",
    "                        book_recs.append(searched_book_title)\n",
    "        return book_recs[:10]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-01T00:47:28.448520Z",
     "start_time": "2020-02-01T00:47:24.846162Z"
    }
   },
   "outputs": [],
   "source": [
    "recommendation('Narcos', 'tv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T21:12:16.458817Z",
     "start_time": "2020-01-23T21:12:16.452377Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_entities_plot(title):\n",
    "    try:\n",
    "        names = model_df['name']\n",
    "        plots = model_df['plot']\n",
    "        for index,name in enumerate(names):\n",
    "            if name == title:\n",
    "                book_index = index\n",
    "                plot_text = plots[book_index]\n",
    "        \n",
    "        #Visualize the entities\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        doc = nlp(plot_text)\n",
    "        spacy.displacy.serve(doc, style='ent')\n",
    "    except:\n",
    "        print('That name is not in the directory!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T21:12:38.210126Z",
     "start_time": "2020-01-23T21:12:27.561109Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_entities_plot('The Da Vinci Code')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmitzation breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T21:12:20.504865Z",
     "start_time": "2020-01-23T21:12:20.497448Z"
    }
   },
   "outputs": [],
   "source": [
    "def visualize_lemmitization(title):\n",
    "    try:\n",
    "        names = model_df['name']\n",
    "        plots = model_df['plot']\n",
    "        for index,name in enumerate(names):\n",
    "            if name == title:\n",
    "                media_index = index\n",
    "                plot_text = plots[media_index]\n",
    "        \n",
    "        #Visualize the entities\n",
    "        nlp = spacy.load('en_core_web_sm')\n",
    "        explacy.print_parse_info(nlp, plot_text)\n",
    "    except:\n",
    "        print('That name is not in the directory!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T21:12:45.112293Z",
     "start_time": "2020-01-23T21:12:44.308873Z"
    }
   },
   "outputs": [],
   "source": [
    "visualize_lemmitization('Narcos: Mexico')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T03:01:50.851299Z",
     "start_time": "2020-01-22T02:58:30.939911Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "corpus = st.CorpusFromPandas(model_df,\n",
    "                             category_col='media_type',\n",
    "                             text_col='tokens',\n",
    "                             nlp=nlp).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terms most associated with books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-22T03:01:53.496825Z",
     "start_time": "2020-01-22T03:01:50.853706Z"
    }
   },
   "outputs": [],
   "source": [
    "print(list(corpus.get_scaled_f_scores_vs_background().index[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the categories of the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T21:52:55.313721Z",
     "start_time": "2020-01-21T21:52:55.293879Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_builder = st.FeatsFromOnlyEmpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T21:56:07.026446Z",
     "start_time": "2020-01-21T21:54:08.839130Z"
    }
   },
   "outputs": [],
   "source": [
    "empath_corpus = st.CorpusFromParsedDocuments(model_df,\n",
    "                                             category_col='media_type',\n",
    "                                             feats_from_spacy_doc=feat_builder,\n",
    "                                             parsed_col='tokens').build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T21:57:26.151285Z",
     "start_time": "2020-01-21T21:56:39.820591Z"
    }
   },
   "outputs": [],
   "source": [
    "html = st.produce_scattertext_explorer(empath_corpus,\n",
    "                                       category='book',\n",
    "                                       category_name='Books',\n",
    "                                       not_category_name='Netflix',\n",
    "                                       width_in_pixels=1000,\n",
    "                                       metadata=model_df['media_type'],\n",
    "                                       use_non_text_features=True,\n",
    "                                       use_full_doc=True,\n",
    "                                       topic_model_term_lists=feat_builder.get_top_model_term_lists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-21T22:10:49.541237Z",
     "start_time": "2020-01-21T22:10:49.504051Z"
    }
   },
   "outputs": [],
   "source": [
    "open(\"book-v-netflix-empathy.html\", 'wb').write(html.encode('utf-8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
